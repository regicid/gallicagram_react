For the Gallica corpora (all those that have a non-empty value in @public/corpus.tsv in the column Gallica_context_filter), we will add new ways to search. When a Gallica corpus is selected, you add a Search mode dropdown menu with five options: 
* "By ngram" (the default, and the current way, with the shiny-ens-paris-saclay.fr api and the query route.)
* "By document". In this, you use the gallica.bnf.fr api that I will describe now. The route and the parameter is: https://gallica.bnf.fr/SRU?operation=searchRetrieve&exactSearch=True&version=1.2&startRecord=0&maximumRecords=0&collapsing=false. Then, there is a &query= argument. It looks this way: (text adj "libertÃ©") and gallicapublication_date>="1792-05-01" and gallicapublication_date<"1792-06-01" and arkPress adj "cb327986698_date".
 You put dc.type all "monographie" for corpora Livres de Gallica, and dc.type all "fascicule" for corpora Presse de Gallica. For the other corpora, you use the arkPress adj field, with the code value in the column Gallica_context_filter in the @public/corpus.tsv file and you append _date to it. For instance:  arkPress adj "cb327986698_date".
You make one api call for each period (careful if the resolution is monthly or daily, adjust accordingly), and in the return, there is a <srw:numberOfRecords>x</srw:numberOfRecords> tag. You take the value of x for the column 'n' of the resulting table. For this time period, you also make an empty api call to get the total number of documents in the corpus. You take the value of x for the column 'total' of the resulting table. Like in the ngram mode, we will divide the value of 'n' by the value of 'total' to get the percentage of documents that contain the ngram and this is the value for the lineplot or the area plot (in the barplot or sums or wordcloud, we just use the n column).
* By cooccurrence. This is the same thing, but in this case, the user can enter two ngrams and a distance separating them, so adjust the form fields accordingly. Then, the api calls have (( text all "word1" prox/unit=word/distance=n_distance "word2")). Same logic as before. 
* "By Joker". This mode only exists for the presse and livres corpora. When selected, add a new form field for the number of joker words to return (default 10), a field for the length of jokers (default, the number of words in the word field + 1) and a field for stopwords to ignore (default 500).
In this, we use the shiny-ens-paris-saclay.fr api and the joker route. The syntax is this: https://shiny.ens-paris-saclay.fr/guni/joker?mot=camarade&corpus=presse&from=1789&to=1950&n_joker=10&length=2&stopwords=0. This returns a csv with tot and gram response. In this mode you switch to the Sums view, and then you do the same thing as the current view, but no need to aggregate here: the tot view is directly the sum of the number of occurrences. Reuse the existing code as much as possible: give the data the right shape, and let the current implementation handle the plotting.
* "By nearby word" (in French translation "Par mots environnants"). This mode only exists for the presse and livres corpora. When selected, add a new form field for the number of words to return (default 10), and a field for the length of jokers (default, the number of words in the word field + 1). Use the exact same way as the joker mode, but with "associated" route instead of "joker". 
* These two last modes "By Joker" and "By nearby word" also exist when corpus is Le Monde. In this case, you pass corpus=lemonde (not corpus=lemonde_rubriques) instead of corpus=presse or corpus=livres.